experiment:
  name: WDRE
  group: KD/ATAKD/p2
  continue_with_errors: False
  start_from_grid: 0
  start_from_run: 0
  logger: clearml
  tracking_dir: null
  entity: null
  excluded_files: "*.pth"

parameters:
  tags: [[]]
  # train, test, inference
  # phases: [[run]]
  phases: [[train, test]]
  dataset_interface: [wd/data/WeedMapDatasetInterface]

  train_params:
    max_epochs: [500]
    initial_lr: [0.0001]
    optimizer: [Adam]
    optimizer_params:
      weight_decay: [0]
    loss:
      name: [wd/loss/loss/RMILoss]
      params:
        num_classes: [3]
    # ema: True
    seed: [42]
    zero_weight_decay_on_bias_and_bn: [True]
    average_best_models: [False]
    greater_metric_to_watch_is_better: [True]
    metric_to_watch: [F1Score]
    early_stopping_patience: [25]
    freeze_pretrained: [True]
    inform_loss_in_validaiton: [True]

  train_metrics:
    jaccard:
      num_classes: [3]
    f1:
      average: [macro]
      num_classes: [3]
      mdmc_average: [global]
  test_metrics:
    jaccard:
      num_classes: [3]
    conf_mat:
      num_classes: [3]
    f1:
      num_classes: [3]
      average: [macro]
      mdmc_average: [global]
    precision:
      average: [macro]
      num_classes: [3]
      mdmc_average: [global]
    recall:
      average: [macro]
      num_classes: [3]
      mdmc_average: [global]

  kd:
    module:
      name: [logits_distillation]
      params:

    teacher:
      name: [lawin]
      params:
        backbone: [MiT-B0]
        backbone_pretrained: [True]
        main_channels: [5]
        main_pretrained: [['R', 'G', 'B', 'G', 'G']]
        pretrained: 
          run: [ashen-chupacabra-48]

    loss:
      name: [wd/loss/loss/TeacherAdaptiveDistillationLoss]
      params:
        distillation_loss:
          name: [vis_kldiv_loss]
        momentum: [0.0]
        N: [0.1]

  model:
    name: [lawin]
    params:
      backbone: [MiT-L0]

  dataset:
    root: ["dataset/0_rotations_processed_003_test/RedEdge"]
    train_folders: [['000', '001', '002', '004']]
    test_folders: [['003']]
    hor_flip: [True]
    ver_flip: [True]
    channels: [['R', 'G', 'B', 'NIR', 'RE']]
    batch_size: [6]
    val_batch_size: [12]
    test_batch_size: [12]
    num_workers: [0]
    num_classes: [3]
    return_path: [True]
    size: [same]
    crop_size: [same]

  val_callbacks: 
    SegmentationVisualizationCallback:
  test_callbacks: 
    SegmentationVisualizationCallback:


other_grids:
  - kd:
      teacher:
        name: [lawin]
        params:
          pretrained: 
            run: [afraid-xantus-64]
      loss:
        name: [wd/loss/loss/TeacherAdaptiveDistillationLoss]
        params:
          momentum: [0.0]
          N: [0.2]
  - kd:
      teacher:
        name: [lawin]
        params:
          pretrained: 
            run: [unfragrant-antelopegroundsquirrel-16]
      loss:
        name: [wd/loss/loss/TeacherAdaptiveDistillationLoss]
        params:
          momentum: [0.0]
          N: [0.3]
  - kd:
      teacher:
        name: [lawin]
        params:
          pretrained: 
            run: [oozy-needletail-96]
      loss:
        name: [wd/loss/loss/TeacherAdaptiveDistillationLoss]
        params:
          momentum: [0.9]
          N: [0.1]
  - kd:
      teacher:
        name: [lawin]
        params:
          pretrained: 
            run: [endless-kid-78]
      loss:
        name: [wd/loss/loss/TeacherAdaptiveDistillationLoss]
        params:
          momentum: [0.9]
          N: [0.2]
  - kd:
      teacher:
        name: [lawin]
        params:
          pretrained: 
            run: [shorted-jaguarundi-100]
      loss:
        name: [wd/loss/loss/TeacherAdaptiveDistillationLoss]
        params:
          momentum: [0.9]
          N: [0.3]