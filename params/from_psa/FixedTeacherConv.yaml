experiment:
  name: WDRE
  group: KD/FixedTeacher
  continue_with_errors: False
  start_from_grid: 0
  start_from_run: 0
  logger: clearml
  tracking_dir: null
  entity: null
  excluded_files: "*.pth"

parameters:
  tags: [[]]
  # train, test, inference
  # phases: [[run]]
  phases: [[train, test]]
  dataset_interface: [wd/data/WeedMapDatasetInterface]

  train_params:
    max_epochs: [500]
    initial_lr: [0.0001]
    optimizer: [Adam]
    optimizer_params:
      weight_decay: [0]
    loss:
      name: [focal]
      params:
        weight: [[0.0273, 1.0, 4.3802]]
        gamma: [2.0]
    # ema: True
    seed: [42]
    zero_weight_decay_on_bias_and_bn: [True]
    average_best_models: [False]
    greater_metric_to_watch_is_better: [False]
    metric_to_watch: [loss]
    early_stopping_patience: [25]
    pass_targets_to_net: [True]

  train_metrics:
    jaccard:
      num_classes: [3]
    f1:
      average: [macro]
      num_classes: [3]
      mdmc_average: [global]
  test_metrics:
    jaccard:
      num_classes: [3]
    conf_mat:
      num_classes: [3]
    f1:
      num_classes: [3]
      average: [macro]
      mdmc_average: [global]
    precision:
      average: [macro]
      num_classes: [3]
      mdmc_average: [global]
    recall:
      average: [macro]
      num_classes: [3]
      mdmc_average: [global]

  kd:
    module:
      name: [wd/network/kd/FixTeacherDistillationModule]
      params:
        num_classes: [3]

    teacher:
      name: [wd/network/ocrnet/HRNet]
      params:
        pretrained: [mythopoeic-godwit-66]
        side_pretrained: ['G']
        aux_output: [False]

    loss:
      name: [kd_logits_loss]
      params:
        distillation_loss:
          name: [vis_kldiv_loss]
        distillation_loss_coeff: [0.2, 0.5]

  model:
    name: [lawin]
    params:
      backbone: [MiT-L0]

  dataset:
    root: ["dataset/0_rotations_processed_003_test/RedEdge"]
    train_folders: [['000', '001', '002', '004']]
    test_folders: [['003']]
    hor_flip: [True]
    ver_flip: [True]
    channels: [['R', 'G', 'B', 'NIR', 'RE']]
    batch_size: [6]
    val_batch_size: [12]
    test_batch_size: [12]
    num_workers: [0]
    num_classes: [3]
    return_path: [True]
    size: [same]
    crop_size: [same]

  val_callbacks: 
    SegmentationVisualizationCallback:
  test_callbacks: 
    SegmentationVisualizationCallback:


other_grids:
  - kd:
      loss:
        name: [wd/loss/loss/TacherDistillationLoss]
        params:
          distillation_loss:
            name: [vis_kldiv_loss]
          teacher_loss:
              name: [focal]
              params:
                weight: [[0.0273, 1.0, 4.3802]]
                gamma: [2.0]
          distillation_loss_coeff: [0.5, 0.8, 1.0]

